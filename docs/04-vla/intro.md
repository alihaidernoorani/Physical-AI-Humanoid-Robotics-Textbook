---
id: intro
title: "Module 4: Vision-Language-Action (VLA)"
description: "Multimodal reasoning for autonomous humanoids - Voice-to-text, LLM-based task decomposition, and grounding language in ROS 2 action servers"
personalization: true
translation: ur
learning_outcomes:
  - "Implement voice-to-text systems for humanoid interaction"
  - "Create LLM-based task decomposition for robotics"
  - "Ground natural language in ROS 2 action servers"
  - "Build end-to-end autonomous humanoid systems"
software_stack:
  - "OpenAI Whisper or NVIDIA Riva for ASR"
  - "Large Language Models (LLM) integration"
  - "ROS 2 action server framework"
  - "NVIDIA Isaac ROS Foundation Packages"
hardware_recommendations:
  - "NVIDIA Jetson AGX Orin with audio input"
  - "High-quality microphone array"
  - "Cloud connectivity for LLM services"
hardware_alternatives:
  - "Laptop with microphone and cloud access"
  - "NVIDIA Jetson Orin Nano with external audio"
prerequisites:
  - "Module 1-3: Complete understanding of ROS 2, simulation, and AI components"
assessment_recommendations:
  - "Integration test: Voice command to action execution"
  - "Performance evaluation: End-to-end system validation"
dependencies: ["01-ros2-nervous-system", "02-digital-twin", "03-ai-robot-brain"]
---

# Module 4: Vision-Language-Action (VLA)

This module covers multimodal reasoning for autonomous humanoids with end-to-end integration.